<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> â€“ Concepts</title>
    <link>https://docs.storageos.com/docs/concepts/</link>
    <description>Recent content in Concepts on </description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://docs.storageos.com/docs/concepts/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Architecture</title>
      <link>https://docs.storageos.com/docs/concepts/architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.storageos.com/docs/concepts/architecture/</guid>
      <description>
        
        
        &lt;p&gt;StorageOS is a software-defined storage platform for running stateful
applications in Kubernetes.&lt;/p&gt;
&lt;p&gt;Fundamentally, StorageOS uses the storage attached to the nodes in the
StorageOS cluster to create and present virtual volumes into containers. Space
on the host is consumed from the mount point &lt;code&gt;/var/lib/storageos/data&lt;/code&gt;, so it
is therefore recommended that disk devices are used exclusively for StorageOS,
as described in &lt;a href=&#34;https://docs.storageos.com/docs/operations/managing-host-storage/&#34;&gt;Managing Host Storage &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;StorageOS is agnostic to the underlying storage and runs equally well on
bare metal, in virtual machines or on cloud providers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://docs.storageos.com/images/docs/concepts/storageos-cluster.png&#34; alt=&#34;StorageOS architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;Read about &lt;a href=&#34;https://storageos.com/storageos-cloud-native-storage&#34;&gt;the cloud native storage principles behind
StorageOS&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;storageos-on-kubernetes&#34;&gt;StorageOS on Kubernetes&lt;/h3&gt;
&lt;p&gt;StorageOS is architected as a series of containers that fulfil separate,
discrete functions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;StorageOS Controlplane&lt;/p&gt;
&lt;p&gt;Responsible for monitoring and maintaining the state of volumes and nodes in the cluster&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;StorageOS Dataplane&lt;/p&gt;
&lt;p&gt;Responsible for all I/O path related tasks; reading, writing, compression and caching&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;StorageOS Scheduler&lt;/p&gt;
&lt;p&gt;Responsible for scheduling applications on the same node as applications volume&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CSI helper&lt;/p&gt;
&lt;p&gt;Responsible for registering StorageOS with Kubernetes as a CSI driver&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;StorageOS Operator&lt;/p&gt;
&lt;p&gt;Responsible for the creation and maintenance of the StorageOS cluster&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StorageOS is deployed by the StorageOS operator. In Kubernetes, the StorageOS
Controlplane and Dataplane are deployed in a single pod managed by a daemonset.
This daemonset runs on every node in the cluster that will consume or present
storage. The scheduler, CSI helper and Operator run as separate pods and are
controlled as deployments.&lt;/p&gt;
&lt;p&gt;StorageOS is designed to feel familiar to Kubernetes and Docker users. Storage
is managed through standard StorageClasses and PersistentVolumeClaims, and
&lt;a href=&#34;https://docs.storageos.com/docs/reference/labels/&#34;&gt;features&lt;/a&gt; are controlled by
Kubernetes-style labels and selectors, prefixed with &lt;code&gt;storageos.com/&lt;/code&gt;. By
default, volumes are cached to improve read performance and compressed to
reduce network traffic.&lt;/p&gt;
&lt;p&gt;Any pod may mount a StorageOS virtual volume from any node that is also
running StorageOS, regardless of whether the pod and volume are
collocated on the same node. Therefore, applications may be started or
restarted on any node and access volumes transparently.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Clusters</title>
      <link>https://docs.storageos.com/docs/concepts/clusters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.storageos.com/docs/concepts/clusters/</guid>
      <description>
        
        
        &lt;p&gt;StorageOS clusters represent groups of nodes which run a common distributed
control plane.&lt;/p&gt;
&lt;p&gt;Typically, a StorageOS cluster maps one-to-one to a Kubernetes (or similar
orchestrator) cluster, and we expect our daemonset to run on all worker
nodes within the cluster that will consume or present storage.&lt;/p&gt;
&lt;p&gt;Clusters use etcd to maintain state and manage distributed consensus between
nodes.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Compression</title>
      <link>https://docs.storageos.com/docs/concepts/compression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.storageos.com/docs/concepts/compression/</guid>
      <description>
        
        
        &lt;p&gt;StorageOS compression is handled on a per volume basis and is disabled by
default in v2.2+, as performance is generally increased when compression is
disabled due to block alignment. This means that there is a trade
off between volume performance and the space the volume occupies on the backend
device.&lt;/p&gt;
&lt;p&gt;Compression can be enabled by setting the &lt;a href=&#34;https://docs.storageos.com/docs/reference/labels&#34;&gt;label&lt;/a&gt;
&lt;code&gt;storageos.com/nocompress=false&lt;/code&gt; on a volume at volume creation time.&lt;/p&gt;
&lt;p&gt;StorageOS utilises the &lt;a href=&#34;https://lz4.github.io/lz4/&#34;&gt;lz4 compression algorithm&lt;/a&gt;
when writing to the backend store and when compressing &lt;a href=&#34;https://docs.storageos.com/docs/concepts/replication&#34;&gt;replication
traffic&lt;/a&gt; before it is sent across the network.&lt;/p&gt;
&lt;p&gt;StorageOS detects whether a block can be compressed or not by creating a
heuristic that predicts the size of a compressed block. If the heuristic
indicates that the compressed block is likely to be larger than the
original block then the uncompressed block is stored. Block size increases post
compression if the compression dictionary is added to a block that cannot be
compressed. By verifying whether blocks can be compressed, disk efficiency is
increased and CPU resources are not wasted on attempts to compress
uncompressible blocks. StorageOS&amp;rsquo; patented on disk format is used to tell
whether individual blocks are compressed without overhead. As such volume
compression can be dynamically enabled/disabled even while a volume is in use.&lt;/p&gt;
&lt;p&gt;When compression and &lt;a href=&#34;https://docs.storageos.com/docs/concepts/encryption&#34;&gt;encryption&lt;/a&gt; are both enabled
for a volume, blocks are compressed then encrypted.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Etcd</title>
      <link>https://docs.storageos.com/docs/concepts/etcd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.storageos.com/docs/concepts/etcd/</guid>
      <description>
        
        
        &lt;p&gt;&lt;a href=&#34;https://etcd.io&#34;&gt;Etcd&lt;/a&gt; is an open-source distributed, strongly consistent key
value store that is used by StorageOS to durably persist the StorageOS cluster
state. As the backing store for Kubernetes, StorageOS uses etcd for many of the
same reasons.&lt;/p&gt;
&lt;p&gt;StorageOS uses etcd as the single source of truth for all StorageOS objects.
Whenever a request is made to create, update or delete an object the result is
written to etcd before the request is completed. Using etcd as a configuration
store allows nodes to retrieve the current cluster state after being offlined,
allowing offlined nodes to rejoin the cluster.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;N.B. StorageOS v2.0 does not provide an embedded etcd server as previous
versions did. You will need to setup an etcd server for StorageOS to use
prior to installation of StorageOS. Please see our &lt;a href=&#34;https://docs.storageos.com/docs/prerequisites/etcd/&#34;&gt;etcd prerequisites&lt;/a&gt; page for more information on how
to install and configure etcd.&lt;/p&gt;
&lt;/blockquote&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Namespaces</title>
      <link>https://docs.storageos.com/docs/concepts/namespaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.storageos.com/docs/concepts/namespaces/</guid>
      <description>
        
        
        &lt;p&gt;StorageOS namespaces are an identical concept to Kubernetes namespaces. They
are intended to allow a StorageOS cluster to be used by multiple teams across
multiple projects.&lt;/p&gt;
&lt;p&gt;It is not necessary to create StorageOS namespaces manually, as StorageOS maps
Kubernetes namespaces on a one-to-one basis when PersistentVolumeClaims using
the StorageOS StorageClass are created.&lt;/p&gt;
&lt;p&gt;Access to Namespaces is controlled through user or group level &lt;a href=&#34;https://docs.storageos.com/docs/concepts/policies/&#34;&gt;policies&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Nodes</title>
      <link>https://docs.storageos.com/docs/concepts/nodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.storageos.com/docs/concepts/nodes/</guid>
      <description>
        
        
        &lt;p&gt;A StorageOS node is any machine (virtual or physical) that is running the
StorageOS daemonset pod. A node must be running a daemonset pod in order to
consume and/or present storage.&lt;/p&gt;
&lt;p&gt;Nodes can be run in several modes.&lt;/p&gt;
&lt;h2 id=&#34;hyperconverged-mode&#34;&gt;Hyperconverged Mode&lt;/h2&gt;
&lt;p&gt;By default StorageOS nodes run in &lt;code&gt;hyperconverged&lt;/code&gt; mode. This means that the
node hosts data from StorageOS volumes and can present volumes to applications.&lt;/p&gt;
&lt;p&gt;A hyperconverged node can store data from a volume and present volumes to
applications regardless of whether the data for the volume consumed is placed
on that node or is being served remotely. Remote volumes like this are handled
by an internal protocol to present block device access to applications running
on different nodes from the one to which their backing data store is attached.&lt;/p&gt;
&lt;p&gt;StorageOS implements an extension of a Kubernetes Scheduler object that
influences the placement of Pods on the same nodes as their data.&lt;/p&gt;
&lt;h2 id=&#34;compute-only-mode&#34;&gt;Compute-only Mode&lt;/h2&gt;
&lt;p&gt;Alternatively, a node can run in &lt;code&gt;computeonly&lt;/code&gt; mode, which means no storage is
consumed on the node itself and the node only presents volumes hosted by
other nodes. Volumes presented to applications running on compute only nodes
are therefore all remote. Compute only nodes can be very useful for topologies
where nodes are ephemeral and should not host data, but the ephemeral nodes
host applications that require StorageOS volumes. The nodes that are not
intended to hold data, but just to present StorageOS volumes, can be set as
&lt;code&gt;computeonly&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A node can be marked as compute only at any point in time by adding the label
&lt;code&gt;storageos.com/computeonly=true&lt;/code&gt;, following the &lt;a href=&#34;https://docs.storageos.com/docs/reference/labels/&#34;&gt;labels reference&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;storage-mode&#34;&gt;Storage Mode&lt;/h2&gt;
&lt;p&gt;Finally, nodes can be set to storage mode. Nodes set to storage mode don&amp;rsquo;t
present data locally - instead all data is accessed through the network. This
topology is enforced by tainting the relevant nodes to ensure that application
workloads cannot be scheduled there.&lt;/p&gt;
&lt;p&gt;This mode is ideal for ensuring maximum stability of data access as the node is
isolated from resource drains that may occur due to applications running
alongside. For redundancy purposes, in high load clusters it is ideal to have
several nodes running in this mode.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Policies</title>
      <link>https://docs.storageos.com/docs/concepts/policies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.storageos.com/docs/concepts/policies/</guid>
      <description>
        
        
        &lt;p&gt;StorageOS policies are a way to control user and group access to StorageOS
&lt;a href=&#34;https://docs.storageos.com/docs/concepts/namespaces/&#34;&gt;Namespaces&lt;/a&gt;. To grant a user or group
access to a namespace, a policy needs to be created mapping the user or group
to the namespace.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Users always have access to the default namespace&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For more information on how to use policies, see the
&lt;a href=&#34;https://docs.storageos.com/docs/operations/policies/&#34;&gt;Policies operations&lt;/a&gt; page.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: ReadWriteMany</title>
      <link>https://docs.storageos.com/docs/concepts/rwx/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.storageos.com/docs/concepts/rwx/</guid>
      <description>
        
        
        &lt;blockquote&gt;
&lt;p&gt;Please note: StorageOS Project edition is required to create RWX Volumes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;StorageOS supports ReadWriteMany (RWX) &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes&#34;&gt;access
mode&lt;/a&gt;
Persistent Volumes. A RWX PVC can be used simultaneously by many Pods in the
same Kubernetes namespace for read and write operations.&lt;/p&gt;
&lt;p&gt;StorageOS RWX Volumes are based on a shared filesystem - in the case of our
implementation, this is NFS.&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;For each RWX Volume, the following components are involved:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;StorageOS ReadWriteOnly (RWO) Volume&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;StorageOS provisions a standard &lt;a href=&#34;https://docs.storageos.com/docs/concepts/volumes/&#34;&gt;Volume&lt;/a&gt; that provides a block device for the file system of the NFS server. This
means that every RWX Volume has its own RWO Volume. This allows RWX Volumes to
leverage the synchronous replication and automatic failover functionality of
StorageOS, providing the NFS server with high availability.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NFS-Ganesha server&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For each RWX Volume, an NFS-Ganesha server is spawned by StorageOS. The NFS
server runs in user space on the Node containing the primary Volume. Each NFS
server uses its own RWO Volume to store data so the data of each Volume is
isolated.&lt;/p&gt;
&lt;p&gt;StorageOS binds an ephemeral port to the host network interface for each
NFS-Ganesha server. The NFS export is presented using NFS v4.2. Check the
&lt;a href=&#34;https://docs.storageos.com/docs/prerequisites/firewalls/&#34;&gt;prerequisites page&lt;/a&gt; to see the
range of ports needed for StorageOS RWX Volumes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;StorageOS API Manager&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;StorageOS fully integrates with Kubernetes. The StorageOS API Manager Pod
monitors StorageOS RWX Volumes to create and maintain a Kubernetes Service
that points towards each RWX Volume&amp;rsquo;s NFS export endpoint. The API Manager is
responsible for updating the Service endpoint when a RWX Volume failover
occurs.&lt;/p&gt;
&lt;h2 id=&#34;provisioning-and-using-rwx-pvcs&#34;&gt;Provisioning and using RWX PVCs&lt;/h2&gt;
&lt;p&gt;The sequence in which a RWX PVC is provisioned and used is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A PersistentVolumeClaim (PVC) is created with RWX access mode using any
StorageOS StorageClass.&lt;/li&gt;
&lt;li&gt;StorageOS dynamically provisions the PV.&lt;/li&gt;
&lt;li&gt;A new StorageOS RWO Volume is provisioned internally (not visible in
Kubernetes).&lt;/li&gt;
&lt;li&gt;When the RWX PVC is consumed by a pod, an NFS-Ganesha server is instantiated
on the same Node as the primary Volume. The NFS-Ganesha server thus uses the
RWO StorageOS Volume as its back end disk.&lt;/li&gt;
&lt;li&gt;The StorageOS API Manager publishes the host IP and port for the NFS service
endpoint, by creating a Kubernetes Service that points to the NFS-Ganesha
server export endpoint.&lt;/li&gt;
&lt;li&gt;StorageOS issues a NFS mount on the Node where the Pod using the PVC is
scheduled.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;high-availability&#34;&gt;High availability&lt;/h2&gt;
&lt;p&gt;RWX Volumes failover in the same way as standard RWO StorageOS Volumes. The
replica Volume is promoted upon detection of Node failure and the NFS-Ganesha
server is started on the Node containing the promoted replica. The StorageOS
API Manager updates the endpoint of the Volume&amp;rsquo;s NFS service, causing traffic
to be routed to the URL of the new NFS-Ganesha server. The NFS client in the
application Node (where the user&amp;rsquo;s Pod is running) automatically reconnects.&lt;/p&gt;
&lt;h2 id=&#34;notes&#34;&gt;Notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;All feature labels that work on RWO Volumes will also work on RWX Volumes.&lt;/li&gt;
&lt;li&gt;A StorageOS RWX Volume is matched one-to-one with a PVC. Therefore the
StorageOS RWX Volume can only be accessed by Pods in the same Kubernetes
namespace.&lt;/li&gt;
&lt;li&gt;StorageOS RWX Volumes support volume resize. Refer to the &lt;a href=&#34;https://docs.storageos.com/docs/operations/resize/&#34;&gt;resize&lt;/a&gt; documentation for more details.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Replication</title>
      <link>https://docs.storageos.com/docs/concepts/replication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.storageos.com/docs/concepts/replication/</guid>
      <description>
        
        
        &lt;p&gt;StorageOS replicates volumes across nodes for data protection and high
availability. Synchronous replication ensures strong consistency for
applications such as databases and Elasticsearch, incurring one network round
trip on writes.&lt;/p&gt;
&lt;p&gt;The basic model for StorageOS replication is of a master volume with distributed
replicas. Each volume can be replicated between 0 and 5 times, which are
provisioned to 0 to 5 nodes, up to the number of remaining nodes in the cluster.&lt;/p&gt;
&lt;p&gt;In this diagram, the master volume &lt;code&gt;D&lt;/code&gt; was created on node 1, and two replicas,
&lt;code&gt;D2&lt;/code&gt; and &lt;code&gt;D3&lt;/code&gt; on nodes 3 and 5.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://docs.storageos.com/images/docs/concepts/high-availability.png&#34; alt=&#34;StorageOS replication&#34;&gt;&lt;/p&gt;
&lt;p&gt;Writes that come into &lt;code&gt;D&lt;/code&gt; (step 1) are written in parallel to &lt;code&gt;D2&lt;/code&gt; and &lt;code&gt;D3&lt;/code&gt;
(step 2). When both replicas and the master acknowledge that the data has been
written (step 3), the write operation return successfully to the application
(step 4).&lt;/p&gt;
&lt;p&gt;For most applications, one replica is sufficient (&lt;code&gt;storageos.com/replicas=1&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;All replication traffic on the wire is compressed using the lz4 algorithm, then
streamed over tcp/ip to target port tcp/5703.&lt;/p&gt;
&lt;p&gt;If the master volume is lost, a replica is promoted to master (&lt;code&gt;D2&lt;/code&gt; or &lt;code&gt;D3&lt;/code&gt;
above) and a new replica is created and synced on an available node (Node 2 or
4). This is transparent to the application and does not cause downtime.&lt;/p&gt;
&lt;p&gt;If a replica volume is lost and there are enough remaining nodes, a new replica
is created and synced on an available node. While a new replica is created and
being synced, the volume&amp;rsquo;s health will be marked as degraded.&lt;/p&gt;
&lt;p&gt;If the lost replica comes back online before the new replica has finished
synchronizing, then StorageOS will calculate which of the two synchronizing
replicas has the smallest difference compared to the master volume and keep
that replica. The same holds true if a master volume is lost and a replica is
promoted to be the new master. If possible, a new replica will be created and
begin to sync. Should the former master come back online it will be demoted to
a replica and the replica will the smallest difference to the current master
will be kept.&lt;/p&gt;
&lt;p&gt;While the replica count is controllable on a per-volume basis, some
environments may prefer to set &lt;a href=&#34;https://docs.storageos.com/docs/reference/labels/#storageos-storageclass-labels&#34;&gt;default labels on the StorageClass&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;number-of-nodes&#34;&gt;Number of nodes&lt;/h3&gt;
&lt;p&gt;StorageOS replicas are distributed across available nodes. When a node fails,
a new replica is provisioned and synced as described above. Failure to
provision a new replica will eventually cause the volume to become read-only.
StorageOS will re-attempt creation of the replica for 90 seconds. After that
period, if the old replica is not available and a new replica cannot be
provisioned, StorageOS cannot guarantee that the data is safe and stored on
multiple nodes as requested by the user. It will therefore force the volume
to be set to read-only.&lt;/p&gt;
&lt;p&gt;To ensure that a new replica can always be created, an additional node should
be available. To guarantee high availability, clusters using volumes with 1
replica must have at least 3 storage nodes. When using volumes with 2 replicas,
at least 4 storage nodes, 3 replicas, 5 nodes, etc.&lt;/p&gt;
&lt;p&gt;Minimum number of storage nodes = 1 (primary) + N (replicas) + 1&lt;/p&gt;
&lt;p&gt;For an application to be able to use the volume normally, the volume needs to
recover from a degraded state. This is achieved by having enough nodes on which
to place the data, and triggering a remount of the volumes - i.e. deleting the
Pod mounting the volume once it is in a healthy state.&lt;/p&gt;
&lt;h3 id=&#34;delta-sync&#34;&gt;Delta Sync&lt;/h3&gt;
&lt;p&gt;StorageOS implements a delta sync between a volume master and its replicas.
This means that if a replica for a volume goes offline, that when the replica
comes back online only the regions with changed blocks need to be synchronized.
This optimization reduces the time it takes for replicas to catch up, improving
volume resilience. Additionally, it reduces network and IO bandwidth which can
reduce costs when running in public clouds.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Volumes</title>
      <link>https://docs.storageos.com/docs/concepts/volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://docs.storageos.com/docs/concepts/volumes/</guid>
      <description>
        
        
        &lt;p&gt;StorageOS volumes are a logical construct which represent a writeable volume
and exhibit standard POSIX semantics. StorageOS presents volumes as mounts into
containers via the Linux LIO subsystem.&lt;/p&gt;
&lt;p&gt;Conceptually, StorageOS volumes have a frontend presentation, which is what
the application sees, and a backend presentation, which is the actual on-disk
format. Depending on the configuration, frontend and backend components may be
on the same or different hosts.&lt;/p&gt;
&lt;p&gt;Volumes are formatted using the linux standard ext4 filesystem by default.
Kubernetes users may change the default filesystem type to ext2, ext3, ext4,
or xfs by setting the fsType parameter in their StorageClass (see
&lt;a href=&#34;https://docs.storageos.com/docs/reference/filesystems#persistent-volume-filesystems&#34;&gt;Supported
Filesystems&lt;/a&gt; for
more information). Different filesystems may be supported in the future.&lt;/p&gt;
&lt;p&gt;StorageOS volumes are represented on disk in two parts. Actual volume data is
written to blob files in &lt;code&gt;/var/lib/storageos/data/dev[\d+]&lt;/code&gt;. Inside these
directories, each StorageOS block device gets two blob files of the form
&lt;code&gt;vol.xxxxxx.y.blob&lt;/code&gt;, where x is the inode number for the device, and y is an
index between 0 and 1. We provide two blob files in order to ensure that
certain operations which require locking do not impede in-flight writes to the
volume.&lt;/p&gt;
&lt;p&gt;In systems which have multiple &lt;code&gt;/var/lib/storageos/data/dev[\d+]&lt;/code&gt; directories,
two blob files are created per block device. This allows us to load-balance
writes across multiple devices. In cases where dev directories are added after
a period of run time, later directories are favoured for writes until the data
is distributed evenly across the blob files.&lt;/p&gt;
&lt;p&gt;Metadata is kept in directories named &lt;code&gt;/var/lib/storageos/data/db[\d+]&lt;/code&gt;. We
maintain an index of all blocks written to the blob file inside the metadata
store, including checksums. These checksums allow us to detect bitrot, and
return errors on reads, rather than serve bad data. In future versions we may
implement recovery from replicas for volumes with one or more replicas defined.&lt;/p&gt;
&lt;p&gt;StorageOS metadata requires approximately 2.7GB of storage per 1TB of allocated
blocks in the associated volume. This size is consistent irrespective of data
compression defined on the volume.&lt;/p&gt;
&lt;p&gt;To ensure deterministic performance, individual StorageOS volumes must fit on a single
node.&lt;/p&gt;
&lt;h2 id=&#34;volume-resize&#34;&gt;Volume Resize&lt;/h2&gt;
&lt;p&gt;StorageOS v2.1 supports offline resize of volumes. This means that a volume
cannot be resized while it is in use. Furthermore, in order for a resize
operation to take place the volume must not be attached to a node. This is to
ensure that the volume is not in use.&lt;/p&gt;
&lt;p&gt;This means that if a Kubernetes pod is currently consuming a volume that a
resize request has been issued for, the resize will not be actioned until the
pod is terminated and the volume is detached from the node. The StorageOS
controlplane will then attach the volume to the node that holds the master
deployment and resize the underlying block device and then run resize2fs to
expand the filesystem.&lt;/p&gt;
&lt;p&gt;For a walk through of how to resize a volume please see the &lt;a href=&#34;https://docs.storageos.com/docs/operations/resize&#34;&gt;Volume
Resize&lt;/a&gt; operations page.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
